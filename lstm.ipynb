{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21808,"status":"ok","timestamp":1727460478046,"user":{"displayName":"A Group","userId":"16248367019244849851"},"user_tz":-480},"id":"e2w6ZHTb8Jbh","outputId":"dc2bb060-bf14-4107-f6fc-63b340f7a9f9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/Colab Notebooks/Group_A_Project\n"]}],"source":["# Mount to Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","# Define Project Folder\n","FOLDERNAME = 'Colab\\ Notebooks/Group_A_Project'\n","\n","%cd drive/MyDrive/$FOLDERNAME"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e2D5Uezf8N67"},"outputs":[],"source":["import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","import pandas as pd\n","from tensorflow.keras.preprocessing.text import Tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1727460484880,"user":{"displayName":"A Group","userId":"16248367019244849851"},"user_tz":-480},"id":"40cylk1U8RvG","outputId":"f8610d8f-46df-4a1d-bbc1-6b970a57d1e1"},"outputs":[{"output_type":"stream","name":"stdout","text":["cuda:0\n"]}],"source":["# Define device\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CB3RJO6b8SQl"},"outputs":[],"source":["# Seed for same output\n","torch.manual_seed(42)\n","torch.backends.cudnn.benchmark = False\n","torch.backends.cudnn.deterministic = True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pn7DugPW8Zli"},"outputs":[],"source":["# Reading in our file\n","raw_data = pd.read_csv('Amazon_Unlocked_Mobile.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vyqFWOQJ9JCx"},"outputs":[],"source":["reviews = raw_data['Reviews']\n","reviews = reviews.astype(str)\n","labels = raw_data['Rating']\n","labels.replace({1:0, 2:0, 3:1, 4:2, 5:2}, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nd_e6imN9MfA"},"outputs":[],"source":["patterns = ['<br />', '--', '.', ',', '!', '?', ')', '(', ';', ':', '*', '~', '_', \"'\", '\"']\n","replacements = [' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', '', '']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6zQKTTWMaTcI"},"outputs":[],"source":["def preprocessing(reviews, patterns, replacements):\n","    lst = []\n","    for i in range(len(reviews)):\n","        review = reviews[i].lower()\n","        for pattern, replacement in zip(patterns, replacements):\n","            review = review.replace(pattern, replacement)\n","        lst.append(review)\n","    return lst"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j02al6c_aU-s"},"outputs":[],"source":["reviews = preprocessing(reviews, patterns, replacements)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DLgPAyqMdSyu"},"outputs":[],"source":["print(reviews)\n","print(labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8b22b_zsfI9R"},"outputs":[],"source":["cleaned_reviews = preprocessing(reviews, patterns, replacements)\n","\n","# Tokenize the reviews and find the length of each tokenized review\n","tokenized_reviews = [review.split() for review in cleaned_reviews]\n","num_tokens_per_review = [len(tokens) for tokens in tokenized_reviews]\n","\n","# Find the longest number of tokens in any review\n","# longest_num_tokens = max(num_tokens_per_review)\n","# print(longest_num_tokens)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-inOncg0abTr"},"outputs":[],"source":["num_train = 244031\n","num_val = 104585\n","longest_num_tokens = 200"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OD1A1mhTa1h7"},"outputs":[],"source":["def indexing_tokens():\n","    indices = {'<SOS>': 0, '<EOS>': 1, '<PAD>': 2, '<UNK>': 3}\n","    index = 4\n","    for i in range(num_train):\n","        review = reviews[i].split()\n","        for token in review:\n","            if token not in indices:\n","                indices[token] = index\n","                index += 1\n","    return indices"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VsnrO_ova44H"},"outputs":[],"source":["def get_data(indices, longest_line_tokens, mode='train'):\n","    data = []\n","    Y = []\n","    if mode == 'train':\n","      for i in range(num_train):\n","        one_train_data = []\n","        y, review = labels[i], reviews[i]\n","        tokens = review.split()\n","        for token in tokens:\n","          one_train_data.append(indices[token])\n","          if len(one_train_data) == longest_line_tokens:\n","            break\n","        while len(one_train_data) < longest_line_tokens:\n","          one_train_data.append(indices['<PAD>'])\n","        one_train_data.insert(0, indices['<SOS>'])\n","        one_train_data.append(indices['<EOS>'])\n","        data.append(one_train_data)\n","        Y.append(y)\n","    else:\n","      for i in range(num_train, num_train+num_val):\n","        one_val_data = []\n","        y, review = labels[i], reviews[i]\n","        tokens = review.split()\n","        for token in tokens:\n","          if token in indices:\n","            one_val_data.append(indices[token])\n","          else:\n","            one_val_data.append(indices['<UNK>'])\n","          if len(one_val_data) == longest_line_tokens:\n","            break\n","        while len(one_val_data) < longest_line_tokens:\n","          one_val_data.append(indices['<PAD>'])\n","        one_val_data.insert(0, indices['<SOS>'])\n","        one_val_data.append(indices['<EOS>'])\n","        data.append(one_val_data)\n","        Y.append(y)\n","    return data, Y"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ktbi4by4bCmn"},"outputs":[],"source":["# Loading Training Data & Val Data\n","indices = indexing_tokens()\n","training_data, training_labels = get_data(indices, longest_num_tokens)\n","val_data, val_labels = get_data(indices, longest_num_tokens, mode='val')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":321,"status":"ok","timestamp":1727444305955,"user":{"displayName":"A Group","userId":"16248367019244849851"},"user_tz":-480},"id":"pufrhnmkbIoU","outputId":"d4cc0366-84f4-442f-a8c6-a4bdd30ee860"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of training: 244031\n","Number of validation: 104585\n","Length of corpus: 56793\n"]}],"source":["print('Number of training:', len(training_data))\n","print('Number of validation:', len(val_data))\n","print('Length of corpus:', len(indices))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B24foIpybLMj"},"outputs":[],"source":["# Create tensors of train & val\n","import numpy as np\n","train_tensor = torch.tensor(np.array(training_data))\n","train_labels_tensor = torch.tensor(np.array(training_labels))\n","val_tensor = torch.tensor(np.array(val_data))\n","val_labels_tensor = torch.tensor(np.array(val_labels))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LongcEMmbLyq"},"outputs":[],"source":["print('Train Tensor:', train_tensor.shape)\n","print('Val Tensor:', val_tensor.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"npD-cFmHbR7Z"},"outputs":[],"source":["vocab_size = 56793\n","embedding_dim = 300\n","hidden_dim = 256\n","sequence_len = 202\n","output_dim = 3\n","print_every = 400\n","batch_size = 64"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4o8K2RxebkGo"},"outputs":[],"source":["class MyModel(nn.Module):\n","  def __init__(self, corpus_len, embedding_dim, hidden_dim, output_dim):\n","    super().__init__()\n","    self.embedding = nn.Embedding(corpus_len, embedding_dim)\n","    self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n","    self.fc = nn.Linear(hidden_dim, output_dim)\n","\n","  def forward(self, x):\n","    out = self.embedding(x)\n","    output, (h_n, c_n) = self.lstm(out)\n","    # out = self.fc(output[:, -1, :])\n","    out = self.fc(h_n).squeeze()\n","    return out"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KU7KRSTVbsFj"},"outputs":[],"source":["model = MyModel(vocab_size, embedding_dim, hidden_dim, output_dim)\n","model = model.cuda()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C4fNCeAYbvZ4"},"outputs":[],"source":["mini_trains = DataLoader(train_tensor, batch_size=batch_size)\n","mini_train_labels = DataLoader(training_labels, batch_size=batch_size)\n","\n","mini_vals = DataLoader(val_tensor, batch_size=batch_size)\n","mini_val_labels = DataLoader(val_labels, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t-50tCR6bxXq","executionInfo":{"status":"ok","timestamp":1727254957206,"user_tz":-480,"elapsed":11,"user":{"displayName":"A Group","userId":"16248367019244849851"}},"outputId":"00d7369e-2974-41b8-fa8e-4ae13e7b27a8"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([64, 202])\n","torch.Size([64])\n"]}],"source":["iterator = iter(mini_trains)\n","print(next(iterator).shape)\n","\n","iterator = iter(mini_train_labels)\n","print(next(iterator).shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dfOiEOe4b25_"},"outputs":[],"source":["loss_function = torch.nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=1e-4)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S3FFdPicbz9r"},"outputs":[],"source":["# Training Procedure\n","def train(num_epoch, model, mini_trains, mini_train_labels, mini_vals, mini_val_labels, device, loss_function, optimizer):\n","    # 用於記錄損失和準確率的列表\n","    train_losses = []\n","    train_accuracies = []\n","    val_losses = []\n","    val_accuracies = []\n","\n","    for epoch in range(num_epoch):\n","        num_iters = 0\n","        total_train_loss = 0\n","        correct_train_preds = 0\n","        total_train_samples = 0\n","\n","        for x, y in zip(mini_trains, mini_train_labels):\n","            model.train()\n","\n","            x = x.to(device)\n","            y = y.to(device)\n","\n","            scores = model(x)\n","            loss = loss_function(scores, y)\n","            total_train_loss += loss.item()\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            predictions = scores.argmax(dim=1)\n","            correct_train_preds += (predictions == y).sum().item()\n","            total_train_samples += y.size(0)\n","\n","            if num_iters % print_every == 0:\n","                print(f\"Iteration {num_iters}, Loss: {loss.item():.4f}\")\n","                evaluate_predictor(model, epoch, mini_vals, mini_val_labels, device, loss_function)\n","            num_iters += 1\n","\n","        # 計算每個 epoch 的訓練準確率和損失\n","        train_accuracy = correct_train_preds / total_train_samples\n","        avg_train_loss = total_train_loss / len(mini_trains)\n","        train_accuracies.append(train_accuracy)\n","        train_losses.append(avg_train_loss)\n","\n","        print(f'Epoch [{epoch + 1}] Training Loss: {avg_train_loss:.4f}, Training Accuracy: {train_accuracy:.4f}')\n","\n","        # 驗證每個 epoch 的 val 準確率跟 loss\n","        val_accuracy, val_loss = evaluate_predictor(model, epoch, mini_vals, mini_val_labels, device, loss_function)\n","        val_accuracies.append(val_accuracy)\n","        val_losses.append(val_loss)\n","\n","        print(f'Epoch [{epoch + 1}] Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wl89X3Elb0uC"},"outputs":[],"source":["# Evaluate Procedure\n","def evaluate_predictor(model, epoch, mini_vals, mini_val_labels, device, loss_function):\n","    model.eval()\n","    with torch.no_grad():\n","        acc_count = 0\n","        total_loss = 0.0\n","        total_samples = 0\n","        for x, y in zip(mini_vals, mini_val_labels):\n","            x = x.to(device)\n","            y = y.to(device)\n","\n","            scores = model(x)\n","            predictions=scores.max(1)[1]\n","\n","            # Calculate the loss for the current batch\n","            loss = loss_function(scores, y)\n","            total_loss += loss.item()\n","\n","            # Update accuracy count\n","            acc = predictions.eq(y).sum().item()\n","            acc_count += acc\n","            total_samples += y.size(0)  # Track the number of samples in the batch\n","\n","    accuracy = acc_count / total_samples\n","    avg_loss = total_loss / len(mini_vals)\n","\n","    # print(f'Epoch [{epoch + 1}] Accuracy: {accuracy:.4f}, Validation Loss: {avg_loss:.4f}')\n","    return accuracy, avg_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6CrF4ZG0b4_U","executionInfo":{"status":"ok","timestamp":1727444814320,"user_tz":-480,"elapsed":475886,"user":{"displayName":"A Group","userId":"16248367019244849851"}},"outputId":"1c418894-eb12-4d0a-b0f5-8ebce66535aa"},"outputs":[{"output_type":"stream","name":"stdout","text":["Iteration 0, Loss: 0.9485\n","Iteration 400, Loss: 0.7269\n","Iteration 800, Loss: 0.8439\n","Iteration 1200, Loss: 1.0232\n","Iteration 1600, Loss: 0.6940\n","Iteration 2000, Loss: 0.7749\n","Iteration 2400, Loss: 0.7437\n","Iteration 2800, Loss: 1.0277\n","Iteration 3200, Loss: 0.9852\n","Iteration 3600, Loss: 0.7045\n","Epoch [1] Training Loss: 0.7907, Training Accuracy: 0.6798\n","Epoch [1] Validation Loss: 0.7819, Validation Accuracy: 0.7039\n","Iteration 0, Loss: 0.7395\n","Iteration 400, Loss: 0.7359\n","Iteration 800, Loss: 0.8402\n","Iteration 1200, Loss: 1.0187\n","Iteration 1600, Loss: 0.6846\n","Iteration 2000, Loss: 0.7739\n","Iteration 2400, Loss: 0.7332\n","Iteration 2800, Loss: 1.0276\n","Iteration 3200, Loss: 0.9783\n","Iteration 3600, Loss: 0.6956\n","Epoch [2] Training Loss: 0.7877, Training Accuracy: 0.6812\n","Epoch [2] Validation Loss: 0.7794, Validation Accuracy: 0.7046\n","Iteration 0, Loss: 0.7453\n","Iteration 400, Loss: 0.7216\n","Iteration 800, Loss: 0.8386\n","Iteration 1200, Loss: 0.6500\n","Iteration 1600, Loss: 0.5327\n","Iteration 2000, Loss: 0.5020\n","Iteration 2400, Loss: 0.6171\n","Iteration 2800, Loss: 0.7329\n","Iteration 3200, Loss: 0.4009\n","Iteration 3600, Loss: 0.5231\n","Epoch [3] Training Loss: 0.6185, Training Accuracy: 0.7677\n","Epoch [3] Validation Loss: 0.4917, Validation Accuracy: 0.8290\n","Iteration 0, Loss: 0.5495\n","Iteration 400, Loss: 0.2542\n","Iteration 800, Loss: 0.4895\n","Iteration 1200, Loss: 0.3620\n","Iteration 1600, Loss: 0.3522\n","Iteration 2000, Loss: 0.4369\n","Iteration 2400, Loss: 0.3865\n","Iteration 2800, Loss: 0.5290\n","Iteration 3200, Loss: 0.4387\n","Iteration 3600, Loss: 0.4878\n","Epoch [4] Training Loss: 0.4255, Training Accuracy: 0.8522\n","Epoch [4] Validation Loss: 0.4682, Validation Accuracy: 0.8314\n","Iteration 0, Loss: 0.5347\n","Iteration 400, Loss: 0.2531\n","Iteration 800, Loss: 0.3759\n","Iteration 1200, Loss: 0.3094\n","Iteration 1600, Loss: 0.3251\n","Iteration 2000, Loss: 0.3862\n","Iteration 2400, Loss: 0.3564\n","Iteration 2800, Loss: 0.3722\n","Iteration 3200, Loss: 0.3861\n","Iteration 3600, Loss: 0.3379\n","Epoch [5] Training Loss: 0.3649, Training Accuracy: 0.8736\n","Epoch [5] Validation Loss: 0.4185, Validation Accuracy: 0.8513\n"]}],"source":["# Start training\n","train(5, model, mini_trains, mini_train_labels, mini_vals, mini_val_labels, device, loss_function, optimizer)"]},{"cell_type":"code","source":["def preprocessing(reviews, patterns, replacements):\n","    lst = []\n","    for i in range(len(reviews)):\n","        review = reviews[i].lower()\n","        for pattern, replacement in zip(patterns, replacements):\n","            review = review.replace(pattern, replacement)\n","        lst.append(review)\n","    return lst"],"metadata":{"id":"Oi3zhdSPSf08"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["macrumors_data = pd.read_csv('APIR_commemt_data.csv')\n","# macrumors_data = pd.read_csv('MacRumors - MacRumors.csv')\n","\n","# 轉換日期格式\n","date = pd.to_datetime(macrumors_data['time'], format='%B %d, %Y %I:%M%p')\n","mr_after_data = macrumors_data[date > '2024-09-10']\n","mr_before_data = macrumors_data[date <= '2024-09-10']\n","\n","print(mr_after_data[0])\n","\n","mr_after_contents = mr_after_data['content'].astype(str).tolist()\n","mr_before_contents = mr_before_data['content'].astype(str).tolist()\n","print(len(mr_after_contents))\n","print(len(mr_before_contents))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iQG0Z8d86jaf","executionInfo":{"status":"ok","timestamp":1727450386659,"user_tz":-480,"elapsed":1138,"user":{"displayName":"A Group","userId":"16248367019244849851"}},"outputId":"fa28164e-b735-42a3-bc34-ab30c14761bf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["20\n","10780\n"]}]},{"cell_type":"code","source":["before = preprocessing(mr_before_contents, patterns, replacements)\n","after = preprocessing(mr_after_contents, patterns, replacements)\n","labels = [0]*10800\n","reviews = after + before"],"metadata":{"id":"WZIs0WNuSk-2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# num_train = 9319\n","# num_val = 36898\n","# longest_num_tokens = 200\n","\n","num_train = 20\n","num_val = 10780\n","longest_num_tokens = 200\n","\n"],"metadata":{"id":"O3emjqTKSpo4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def indexing_tokens():\n","    indices = {'<SOS>': 0, '<EOS>': 1, '<PAD>': 2, '<UNK>': 3}\n","    index = 4\n","    for i in range(num_train):\n","        review = reviews[i].split()\n","        for token in review:\n","            if token not in indices:\n","                indices[token] = index\n","                index += 1\n","    return indices"],"metadata":{"id":"bOhfU1_KTBz0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_data(indices, longest_line_tokens, mode='train'):\n","    data = []\n","    Y = []\n","    if mode == 'train':\n","      for i in range(num_train):\n","        one_train_data = []\n","        y, review = labels[i], reviews[i]\n","        tokens = review.split()\n","        for token in tokens:\n","          one_train_data.append(indices[token])\n","          if len(one_train_data) == longest_line_tokens:\n","            break\n","        while len(one_train_data) < longest_line_tokens:\n","          one_train_data.append(indices['<PAD>'])\n","        one_train_data.insert(0, indices['<SOS>'])\n","        one_train_data.append(indices['<EOS>'])\n","        data.append(one_train_data)\n","        Y.append(y)\n","    else:\n","      for i in range(num_train, num_train+num_val):\n","        one_val_data = []\n","        y, review = labels[i], reviews[i]\n","        tokens = review.split()\n","        for token in tokens:\n","          if token in indices:\n","            one_val_data.append(indices[token])\n","          else:\n","            one_val_data.append(indices['<UNK>'])\n","          if len(one_val_data) == longest_line_tokens:\n","            break\n","        while len(one_val_data) < longest_line_tokens:\n","          one_val_data.append(indices['<PAD>'])\n","        one_val_data.insert(0, indices['<SOS>'])\n","        one_val_data.append(indices['<EOS>'])\n","        data.append(one_val_data)\n","        Y.append(y)\n","    return data, Y"],"metadata":{"id":"sNnLP8qHTCZx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loading Training Data & Val Data\n","indices = indexing_tokens()\n","training_data, training_labels = get_data(indices, longest_num_tokens)\n","val_data, val_labels = get_data(indices, longest_num_tokens, mode='val')\n","print(len(training_data))\n","print(len(val_data))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eGlMJkJmTKUH","executionInfo":{"status":"ok","timestamp":1727450421188,"user_tz":-480,"elapsed":972,"user":{"displayName":"A Group","userId":"16248367019244849851"}},"outputId":"4fcf5bc3-d0f8-4a97-f3ef-3be049df7daf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["20\n","10780\n"]}]},{"cell_type":"code","source":["# Create tensors of train & val\n","import numpy as np\n","train_tensor = torch.tensor(np.array(training_data))\n","train_labels_tensor = torch.tensor(np.array(training_labels))\n","val_tensor = torch.tensor(np.array(val_data))\n","val_labels_tensor = torch.tensor(np.array(val_labels))\n","\n","print(len(train_tensor))\n","print(len(val_tensor))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uwV5frHf0-5p","executionInfo":{"status":"ok","timestamp":1727450423722,"user_tz":-480,"elapsed":452,"user":{"displayName":"A Group","userId":"16248367019244849851"}},"outputId":"18c1297c-0a5b-470b-f4eb-e386980458d2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["20\n","10780\n"]}]},{"cell_type":"code","source":["mini_trains = DataLoader(train_tensor, batch_size=batch_size)\n","mini_train_labels = DataLoader(training_labels, batch_size=batch_size)\n","\n","mini_vals = DataLoader(val_tensor, batch_size=batch_size)\n","mini_val_labels = DataLoader(val_labels, batch_size=batch_size)\n","\n","\n","print(mini_trains)\n","\n","print(mini_vals)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3t_hFj_QAlE6","executionInfo":{"status":"ok","timestamp":1727450426317,"user_tz":-480,"elapsed":345,"user":{"displayName":"A Group","userId":"16248367019244849851"}},"outputId":"7f1f9dca-23cc-4d7d-b9d3-579e5ec835c0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<torch.utils.data.dataloader.DataLoader object at 0x7c7645d02cb0>\n","<torch.utils.data.dataloader.DataLoader object at 0x7c7645d02c50>\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","\n","def save_predictions(predictions, comments, filename='APIR_commemt_data_IMDb_after.csv'):\n","    # Create a DataFrame from predictions and comments\n","    df = pd.DataFrame({\n","        'prediction': predictions,\n","        'comment': comments\n","    })\n","\n","    # Save the DataFrame to a CSV file\n","    df.to_csv(filename, index=False)\n","\n","# Example usage\n","# Assuming you have your predictions and comments\n","mr_predicted_results_after = predictor(model, 5, mini_trains, device, loss_function)\n","mr_predicted_results_before = predictor(model, 5, mini_vals, device, loss_function)\n","\n","save_predictions(mr_predicted_results_after, mr_after_contents)\n","save_predictions(mr_predicted_results_before, mr_before_contents, filename='APIR_commemt_data_IMDb_before.csv')"],"metadata":{"id":"SdV5fXvFU6hk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def predictor(model, epoch, mini_vals, device, loss_function):\n","    model.eval()\n","    with torch.no_grad():\n","        total_loss = 0.0\n","        all_predictions = []  # Initialize a list to store predictions\n","\n","        for x in mini_vals:\n","            x = x.to(device)\n","\n","            scores = model(x)\n","            predictions = scores.max(1)[1]  # Get the predicted class indices\n","\n","            # Append predictions to the list\n","            all_predictions.extend(predictions.cpu().numpy().tolist())  # Convert to list and add to all_predictions\n","\n","            # Calculate the loss for the current batch\n","            # Assuming you have the true labels available for loss calculation\n","            # loss = loss_function(scores, y)  # You can include this if you have labels for loss\n","            # total_loss += loss.item()\n","\n","    # Optionally, if you still want to calculate the loss, you would need to adjust your inputs\n","    # return all_predictions if you do not want to calculate accuracy or loss\n","    return all_predictions  # Return only predictions\n","\n"],"metadata":{"id":"S_TDB4V3Rw2M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","# List of your CSV files\n","csv_files = ['MacRumors - MacRumors_IMDb_after.csv', 'MacRumors - MacRumors_IMDb_before.csv', 'Trustpilot_commemt_data_IMDb_after.csv', 'Trustpilot_commemt_data_IMDb_before.csv', 'APIR_commemt_data_IMDb_after.csv', 'APIR_commemt_data_IMDb_before.csv']\n","\n","# Read and concatenate all CSV files into one DataFrame\n","merged_df = pd.concat([pd.read_csv(file) for file in csv_files])\n","\n","# Save the merged DataFrame to a new CSV file\n","merged_df.to_csv('IMDb_output.csv', index=False)\n","\n","df_before = pd.read_csv('IMDb_output.csv')\n","print(df_before)\n","df = pd.read_csv('Updated_Data_with_Year_and_Month (1).csv')\n","\n","\n","df = df.drop(columns=['Content'])\n","\n","# print(df['Time'])\n","# print(df['Month'])\n","# print(df['Year'])\n","\n","df = pd.concat([df, df['Time']], ignore_index=True)\n","df = pd.concat([df, df['Month']], ignore_index=True)\n","df = pd.concat([df, df['Year']], ignore_index=True)\n","df = pd.concat([df, df_before['comment']], ignore_index=True)\n","df = pd.concat([df, df_before['prediction']], ignore_index=True)\n","print(df)\n","\n","\n","# # Strip any leading or trailing spaces from the 'Time' column\n","# df['Time'] = df['Time'].str.strip()\n","\n","# # Convert the 'Time' column to datetime format, handling errors\n","# # df['Time'] = pd.to_datetime(df['Time'], errors='coerce')\n","\n","# # Check for any rows where the conversion failed\n","# if df['Time'].isnull().any():\n","#     print(\"Warning: Some dates could not be converted. They will be set to NaT.\")\n","\n","# # Add 'Year' and 'Month' columns\n","# df['Year'] = df['Time'].dt.year\n","# df['Month'] = df['Time'].dt.month\n","\n","# # Drop the 'Content' column\n","# print(df)\n","\n","\n","# Save the modified DataFrame to a new CSV file\n","df.to_csv('IMDb_output.csv', index=False)\n"],"metadata":{"id":"hcHqaph-Rx1a","executionInfo":{"status":"ok","timestamp":1727458815806,"user_tz":-480,"elapsed":2514,"user":{"displayName":"A Group","userId":"16248367019244849851"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"53c71ab2-dceb-4960-f031-dee8c89f5f18"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["       prediction                                            comment\n","0               0  What's the desirability of a (UK based) 2019 M...\n","1               2  You don’t mention any prices…..Personally I ha...\n","2               0  First off. If you do decide to get this, the8T...\n","3               0  Why do you say this❓You can easily add a myria...\n","4               0  It is possible, but has also caused a lot of i...\n","...           ...                                                ...\n","57109           0  \\nPhilips Hue Play HDMI Sync Box will coordina...\n","57110           0  \\nNBC 'Peacock' streaming service expected in ...\n","57111           0  \\nApple car could be recharged by a vehicle-se...\n","57112           0  \\nShould you upgrade to iPhone 11 Pro if you h...\n","57113           0  \\nApple argues that $14.4B EU tax order 'defie...\n","\n","[57114 rows x 2 columns]\n","              Time    Year  Month    0\n","0       2024-09-11  2024.0    9.0  NaN\n","1       2024-09-11  2024.0    9.0  NaN\n","2       2024-09-11  2024.0    9.0  NaN\n","3       2024-09-11  2024.0    9.0  NaN\n","4       2024-09-11  2024.0    9.0  NaN\n","...            ...     ...    ...  ...\n","571135         NaN     NaN    NaN    0\n","571136         NaN     NaN    NaN    0\n","571137         NaN     NaN    NaN    0\n","571138         NaN     NaN    NaN    0\n","571139         NaN     NaN    NaN    0\n","\n","[571140 rows x 4 columns]\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","\n","# List of your CSV files\n","csv_files = [\n","    'MacRumors - MacRumors_IMDb_after.csv',\n","    'MacRumors - MacRumors_IMDb_before.csv',\n","    'Trustpilot_commemt_data_IMDb_after.csv',\n","    'Trustpilot_commemt_data_IMDb_before.csv',\n","    'APIR_commemt_data_IMDb_after.csv',\n","    'APIR_commemt_data_IMDb_before.csv'\n","]\n","print(merged_df.shape)\n","print(new_data.shape)\n","\n","\n","# Read and concatenate all CSV files into one DataFrame\n","merged_df = pd.concat([pd.read_csv(file) for file in csv_files])\n","\n","# Save the merged DataFrame to a new CSV file\n","merged_df.to_csv('IMDb_output.csv', index=False)\n","\n","# Read the combined CSV and the updated CSV with Year and Month\n","df_before = pd.read_csv('IMDb_output.csv')\n","df = pd.read_csv('Updated_Data_with_Year_and_Month (1).csv')\n","\n","# Drop the 'Content' column from the second DataFrame\n","df = df.drop(columns=['Content'])\n","\n","# Create a new DataFrame with the necessary columns\n","new_data = pd.DataFrame({\n","    'Time': df['Time'],\n","    'Month': df['Month'],\n","    'Year': df['Year'],\n","})\n","\n","print(new_data)\n","# Concatenate new_data with the existing merged_df\n","final_df = pd.concat([merged_df, new_data], ignore_index=True)\n","print(final_df)\n","# Save the modified DataFrame to a new CSV file\n","final_df.to_csv('final_IMDb_output.csv', index=False)\n","\n","# Print the final DataFrame\n","print(final_df)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gCp1Y4HFkvgD","executionInfo":{"status":"ok","timestamp":1727459374993,"user_tz":-480,"elapsed":2056,"user":{"displayName":"A Group","userId":"16248367019244849851"}},"outputId":"e7a1c8ed-d985-445b-e720-a1f99678fb84"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(57114, 2)\n","(57114, 3)\n","                      Time  Month  Year\n","0               2024-09-11      9  2024\n","1               2024-09-11      9  2024\n","2               2024-09-11      9  2024\n","3               2024-09-11      9  2024\n","4               2024-09-11      9  2024\n","...                    ...    ...   ...\n","57109  2019-09-19 12:35:00      9  2019\n","57110  2019-09-19 12:35:00      9  2019\n","57111  2019-09-19 12:35:00      9  2019\n","57112  2019-09-19 12:35:00      9  2019\n","57113  2019-09-19 12:35:00      9  2019\n","\n","[57114 rows x 3 columns]\n","        prediction                                            comment  \\\n","0              0.0  What's the desirability of a (UK based) 2019 M...   \n","1              2.0  You don’t mention any prices…..Personally I ha...   \n","2              0.0  First off. If you do decide to get this, the8T...   \n","3              0.0  Why do you say this❓You can easily add a myria...   \n","4              0.0  It is possible, but has also caused a lot of i...   \n","...            ...                                                ...   \n","114223         NaN                                                NaN   \n","114224         NaN                                                NaN   \n","114225         NaN                                                NaN   \n","114226         NaN                                                NaN   \n","114227         NaN                                                NaN   \n","\n","                       Time  Month    Year  \n","0                       NaN    NaN     NaN  \n","1                       NaN    NaN     NaN  \n","2                       NaN    NaN     NaN  \n","3                       NaN    NaN     NaN  \n","4                       NaN    NaN     NaN  \n","...                     ...    ...     ...  \n","114223  2019-09-19 12:35:00    9.0  2019.0  \n","114224  2019-09-19 12:35:00    9.0  2019.0  \n","114225  2019-09-19 12:35:00    9.0  2019.0  \n","114226  2019-09-19 12:35:00    9.0  2019.0  \n","114227  2019-09-19 12:35:00    9.0  2019.0  \n","\n","[114228 rows x 5 columns]\n","        prediction                                            comment  \\\n","0              0.0  What's the desirability of a (UK based) 2019 M...   \n","1              2.0  You don’t mention any prices…..Personally I ha...   \n","2              0.0  First off. If you do decide to get this, the8T...   \n","3              0.0  Why do you say this❓You can easily add a myria...   \n","4              0.0  It is possible, but has also caused a lot of i...   \n","...            ...                                                ...   \n","114223         NaN                                                NaN   \n","114224         NaN                                                NaN   \n","114225         NaN                                                NaN   \n","114226         NaN                                                NaN   \n","114227         NaN                                                NaN   \n","\n","                       Time  Month    Year  \n","0                       NaN    NaN     NaN  \n","1                       NaN    NaN     NaN  \n","2                       NaN    NaN     NaN  \n","3                       NaN    NaN     NaN  \n","4                       NaN    NaN     NaN  \n","...                     ...    ...     ...  \n","114223  2019-09-19 12:35:00    9.0  2019.0  \n","114224  2019-09-19 12:35:00    9.0  2019.0  \n","114225  2019-09-19 12:35:00    9.0  2019.0  \n","114226  2019-09-19 12:35:00    9.0  2019.0  \n","114227  2019-09-19 12:35:00    9.0  2019.0  \n","\n","[114228 rows x 5 columns]\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","\n","# List of your CSV files\n","csv_files = [\n","    'MacRumors - MacRumors_IMDb_after.csv',\n","    'MacRumors - MacRumors_IMDb_before.csv',\n","    'Trustpilot_commemt_data_IMDb_after.csv',\n","    'Trustpilot_commemt_data_IMDb_before.csv',\n","    'APIR_commemt_data_IMDb_after.csv',\n","    'APIR_commemt_data_IMDb_before.csv'\n","]\n","\n","# Read and concatenate all CSV files into one DataFrame\n","merged_df = pd.concat([pd.read_csv(file) for file in csv_files])\n","\n","# Save the merged DataFrame to a new CSV file\n","merged_df.to_csv('IMDb_output.csv', index=False)\n","\n","# Read the combined CSV and the updated CSV with Year and Month\n","df_before = pd.read_csv('IMDb_output.csv')\n","df = pd.read_csv('Updated_Data_with_Year_and_Month (1).csv')\n","\n","# Drop the 'Content' column from the second DataFrame if it exists\n","if 'Content' in df.columns:\n","    df = df.drop(columns=['Content'])\n","\n","# Create a new DataFrame with the necessary columns\n","new_data = pd.DataFrame({\n","    'Time': df['Time'],\n","    'Month': df['Month'],\n","    'Year': df['Year'],\n","})\n","\n","# Ensure the number of rows matches\n","if merged_df.shape[0] != new_data.shape[0]:\n","    print(\"Warning: The number of rows does not match.\")\n","else:\n","    # Reset indices to avoid InvalidIndexError\n","    merged_df.reset_index(drop=True, inplace=True)\n","    new_data.reset_index(drop=True, inplace=True)\n","\n","    # Concatenate merged_df and new_data side by side\n","    final_df = pd.concat([merged_df, new_data], axis=1)\n","\n","    # Rename columns if necessary to ensure correct final structure\n","    final_df.columns = ['prediction', 'comment', 'Time', 'Month', 'Year']\n","\n","    # Check the final DataFrame shape\n","    print(\"Final DF shape:\", final_df.shape)\n","\n","    # Save the modified DataFrame to a new CSV file\n","    final_df.to_csv('final_IMDb_output.csv', index=False)\n","\n","    # Print the final DataFrame\n","    print(final_df)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S0I5RWNtmrIy","executionInfo":{"status":"ok","timestamp":1727459781295,"user_tz":-480,"elapsed":1814,"user":{"displayName":"A Group","userId":"16248367019244849851"}},"outputId":"191c83f3-b968-459a-a3d6-d12bee6ea846"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Final DF shape: (57114, 5)\n","       prediction                                            comment  \\\n","0               0  What's the desirability of a (UK based) 2019 M...   \n","1               2  You don’t mention any prices…..Personally I ha...   \n","2               0  First off. If you do decide to get this, the8T...   \n","3               0  Why do you say this❓You can easily add a myria...   \n","4               0  It is possible, but has also caused a lot of i...   \n","...           ...                                                ...   \n","57109           0  \\nPhilips Hue Play HDMI Sync Box will coordina...   \n","57110           0  \\nNBC 'Peacock' streaming service expected in ...   \n","57111           0  \\nApple car could be recharged by a vehicle-se...   \n","57112           0  \\nShould you upgrade to iPhone 11 Pro if you h...   \n","57113           0  \\nApple argues that $14.4B EU tax order 'defie...   \n","\n","                      Time  Month  Year  \n","0               2024-09-11      9  2024  \n","1               2024-09-11      9  2024  \n","2               2024-09-11      9  2024  \n","3               2024-09-11      9  2024  \n","4               2024-09-11      9  2024  \n","...                    ...    ...   ...  \n","57109  2019-09-19 12:35:00      9  2019  \n","57110  2019-09-19 12:35:00      9  2019  \n","57111  2019-09-19 12:35:00      9  2019  \n","57112  2019-09-19 12:35:00      9  2019  \n","57113  2019-09-19 12:35:00      9  2019  \n","\n","[57114 rows x 5 columns]\n"]}]},{"cell_type":"code","source":["# 品牌聲譽評分設定\n","def calculate_score(predicted_result):\n","    nominator = 0\n","    denominator = 0\n","\n","    denominator = len(predicted_result)\n","    # positive_count = sum(1 for result in predicted_result if result == 2)\n","    # negative_count = sum(1 for result in predicted_result if result == 0)\n","    counts = predicted_result.value_counts()\n","    positive_count = counts.get(2)  # Default to 0 if 2 is not found\n","    negative_count = counts.get(0)  # Default to 0 if 0 is not found\n","    nominator = positive_count - negative_count\n","\n","    return (nominator / denominator) * 100"],"metadata":{"id":"Fs4bs4cPoaDL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","mr_predicted_results_before = pd.read_csv('MacRumors - MacRumors_IMDb_before.csv')\n","mr_predicted_results_after = pd.read_csv('MacRumors - MacRumors_IMDb_after.csv')\n","\n","mr_before_score = calculate_score(mr_predicted_results_before['prediction'])\n","mr_after_score = calculate_score(mr_predicted_results_after['prediction'])\n","\n","tp_predicted_results_before = pd.read_csv('Trustpilot_commemt_data_IMDb_before.csv')\n","tp_predicted_results_after = pd.read_csv('Trustpilot_commemt_data_IMDb_after.csv')\n","\n","tp_before_score = calculate_score(tp_predicted_results_before['prediction'])\n","tp_after_score = calculate_score(tp_predicted_results_after['prediction'])\n","\n","ap_predicted_results_before = pd.read_csv('APIR_commemt_data_IMDb_before.csv')\n","ap_predicted_results_after = pd.read_csv('APIR_commemt_data_IMDb_after.csv')\n","\n","ap_before_score = calculate_score(ap_predicted_results_before['prediction'])\n","ap_after_score = calculate_score(ap_predicted_results_after['prediction'])\n","\n","# Read CSV file\n","total_predicted_results = pd.read_csv('final_IMDb_output.csv')\n","\n","total_predicted_results['Time'] = pd.to_datetime(total_predicted_results['Time'], errors='coerce')  # errors='coerce' 將無法解析的值轉為 NaT\n","\n","# Filter results by time\n","before_df = total_predicted_results[(total_predicted_results['Time'] >= '2024-08-01') & (total_predicted_results['Time'] <= '2024-09-10')]\n","after_df = total_predicted_results[total_predicted_results['Time'] > '2024-09-10']\n","\n","\n","# 然後對篩選後的數據計算分數\n","total_before_score = calculate_score(before_df['prediction'])\n","total_after_score = calculate_score(after_df['prediction'])\n","\n","print(mr_before_score)\n","print(mr_after_score)\n","\n","print(tp_before_score)\n","print(tp_after_score)\n","\n","print(ap_before_score)\n","print(ap_after_score)\n","\n","print(total_before_score)\n","print(total_after_score)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aexCALjHoqjr","executionInfo":{"status":"ok","timestamp":1727461923151,"user_tz":-480,"elapsed":1272,"user":{"displayName":"A Group","userId":"16248367019244849851"}},"outputId":"de9d762d-e55f-46ac-c3ef-96269fc7ea4a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["-7.734836576508212\n","-2.3285760274707585\n","-91.17647058823529\n","24.137931034482758\n","-91.02040816326532\n","10.0\n","-11.357340720221606\n","-2.3285760274707585\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}