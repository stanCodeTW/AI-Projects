{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyOurKgdrXUs76oaoY3oU+hi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mmyMIp8R6PUe","executionInfo":{"status":"ok","timestamp":1741583383271,"user_tz":-480,"elapsed":26031,"user":{"displayName":"Group D SC201 OCT24","userId":"05617796782478803590"}},"outputId":"d1480955-7a2b-42c7-e5d4-75b0df061451"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/poster\n"]}],"source":["# Mount to Google Drive\n","from google.colab import drive\n","drive.mount(\"/content/drive\", force_remount=True)\n","FOLDERNAME = \"poster\"\n","%cd drive/MyDrive/$FOLDERNAME"]},{"cell_type":"code","source":["# Load and split CSV\n","import pandas as pd\n","def load_and_split_dataset(csv_file):\n","    df = pd.read_csv(csv_file)\n","    scan_labels = df.set_index('id')['scan-level label']\n","    scan_train_data = df[df['group'] == 'Train'].reset_index(drop=True)\n","    scan_val_data = df[df['group'] == 'Valid'].reset_index(drop=True)\n","    return scan_labels, scan_train_data, scan_val_data\n"],"metadata":{"id":"Z-pi33rX6eOt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Transforms\n","import torchvision.transforms as T\n","SIZE = 224\n","\n","transform = T.Compose([\n","    T.Resize((SIZE, SIZE)),  # Resize to 224x224 for ResNet\n","    T.RandomRotation(degrees=(-10, 10)),  # Small rotation\n","    T.RandomHorizontalFlip(p=0.5),  # Random horizontal flip\n","    T.RandomVerticalFlip(p=0.5),  # Random vertical flip\n","    # T.Normalize(mean=[0.5], std=[0.5])  # Normalize for better training stability\n","])\n"],"metadata":{"id":"4QGl_DM46j7d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from typing_extensions import final\n","import torchvision.transforms as T\n","import pandas as pd\n","import numpy as np\n","import nibabel as nib\n","import os\n","import cv2\n","from torch.utils.data import Dataset, DataLoader\n","\n","class ScanDataset(Dataset):\n","    def __init__(self, folder_path, data_df, scan_labels, num_slices=50, transform=None):\n","        \"\"\"\n","        Args:\n","            folder_path (str): Path to the folder containing NIfTI scans.\n","            data_df (DataFrame): DataFrame with scan IDs.\n","            scan_labels (dict): Mapping from scan IDs to scan-level labels.\n","            target_size (tuple): Target size for resizing slices (H, W, D).\n","            transform (callable, optional): Optional TorchIO transforms.\n","        \"\"\"\n","        self.folder_path = folder_path\n","        self.scan_labels = scan_labels\n","        self.num_slices = num_slices\n","        self.transform = transform\n","        self.scan_ids = data_df['id'].tolist()\n","\n","    def __len__(self):\n","        return len(self.scan_ids)\n","\n","    def __getitem__(self, idx):\n","        scan_id = self.scan_ids[idx]\n","        scan_file = os.path.join(self.folder_path, f\"{scan_id}.nii.gz\")\n","\n","        # Load scan and mask\n","        img = nib.load(scan_file)\n","\n","        data_obj = img.dataobj  # Lazy loading (Do NOT use get_fdata())\n","\n","        # 獲取影像尺寸\n","        H, W, D = img.shape\n","\n","        # 設定適當的 HU window（例如腹部 CT 常見 -100 ~ 400）\n","        window_min, window_max = -100, 400\n","\n","        # 計算切片起始索引\n","        if D < 60:\n","          start_index = round(D * 0.05)\n","        elif D > 130:\n","          start_index = round(D * 0.2)\n","        else:\n","          start_index = round(D * 0.1)\n","\n","        start_index = max(0, start_index)\n","        end_index = min(start_index + self.num_slices, D)\n","\n","        if (end_index - start_index) != self.num_slices:\n","          start_index = max(0, end_index - self.num_slices)\n","          end_index = min(start_index + self.num_slices, D)\n","\n","        # 只讀取需要的切片\n","        data_slice = np.array(data_obj[:, :, start_index:end_index], dtype=np.float32)\n","\n","        # 套用 HU window\n","        data_slice = np.clip(data_slice, window_min, window_max)\n","        data_slice = (data_slice - window_min) / (window_max - window_min)\n","        # data_slice.shape (512, 512, 50)\n","\n","        # Reshape each slice into (512, 512, 1)\n","        slices = np.expand_dims(data_slice, axis=-1)  # New shape: (512, 512, 50, 1)\n","\n","        # Move slices into batch dimension: (50, 512, 512, 1) → (50, 1, 512, 512)\n","        slices = np.moveaxis(slices, -2, 0)  # Now: (50, 512, 512, 1)\n","        slices = np.transpose(slices, (0, 3, 1, 2))  # Final shape: (50, 1, 512, 512)\n","        # slices.shape (50, 1, 512, 512)\n","\n","        # Convert to PyTorch tensor\n","        tensor = torch.tensor(slices, dtype=torch.float32)\n","        # tensor.shape torch.Size([50, 1, 512, 512])\n","\n","        # transform\n","        if self.transform:\n","          transformed_slices = torch.stack([transform(tensor[i]) for i in range(tensor.shape[0])])\n","          # transformed_slices.shape torch.Size([50, 1, 224, 224])\n","\n","        # Get scan-level label\n","        scan_label = torch.tensor(self.scan_labels[scan_id])\n","\n","        return transformed_slices, scan_label"],"metadata":{"id":"mizwKDe66pbN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define device\n","import torch\n","if torch.cuda.is_available():\n","  device = torch.device('cuda')\n","else:\n","  device = torch.device('cpu')\n","print('Device:', device)"],"metadata":{"id":"OEQmUfo_6qqN","executionInfo":{"status":"ok","timestamp":1741583392866,"user_tz":-480,"elapsed":13,"user":{"displayName":"Group D SC201 OCT24","userId":"05617796782478803590"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"f8b07552-d1b3-4eed-dada-0121ec03bf5d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Device: cuda\n"]}]},{"cell_type":"code","source":["csv_path = \"dataset/TrainValid_split.csv\"\n","scan_labels, scan_train_data, scan_val_data = load_and_split_dataset(csv_path)"],"metadata":{"id":"-1uvVuVF6xVF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create datasets\n","folder_path = \"dataset/1_Train,Valid_Image\"\n","scan_train_data = ScanDataset(folder_path=folder_path, data_df=scan_train_data, scan_labels=scan_labels, transform=transform)\n","scan_val_data = ScanDataset(folder_path=folder_path, data_df=scan_val_data, scan_labels=scan_labels, transform=transform)"],"metadata":{"id":"eao56iGS60Zd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["scan_train_data[1][0].shape"],"metadata":{"id":"fviNPohE6381","executionInfo":{"status":"ok","timestamp":1741583399090,"user_tz":-480,"elapsed":5318,"user":{"displayName":"Group D SC201 OCT24","userId":"05617796782478803590"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ab4b5d7c-20d1-439e-aa1b-ce87cd447718"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([50, 1, 224, 224])"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["scan_val_data[1][0].shape"],"metadata":{"id":"PziAyQrS64y9","executionInfo":{"status":"ok","timestamp":1741583401749,"user_tz":-480,"elapsed":2657,"user":{"displayName":"Group D SC201 OCT24","userId":"05617796782478803590"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ca5a39a7-7ba6-41e9-9ccf-141a11c5ca18"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([50, 1, 224, 224])"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["NUM_SCAN_TRAIN = len(scan_train_data)\n","NUM_SCAN_VAL = len(scan_val_data)\n","print(f\"Train dataset size: {NUM_SCAN_TRAIN}\")\n","print(f\"Val dataset size: {NUM_SCAN_VAL}\")"],"metadata":{"id":"to3evQxG6-AV","executionInfo":{"status":"ok","timestamp":1741583401758,"user_tz":-480,"elapsed":8,"user":{"displayName":"Group D SC201 OCT24","userId":"05617796782478803590"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"9533b821-2cd2-4a31-f8e7-b26bdefda78d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train dataset size: 800\n","Val dataset size: 200\n"]}]},{"cell_type":"code","source":["# Create DataLoaders\n","BATCH = 8\n","mini_scan_trains = DataLoader(scan_train_data, batch_size=BATCH, shuffle=True)\n","mini_scan_vals = DataLoader(scan_val_data, batch_size=BATCH, shuffle=False)\n","print(mini_scan_trains)\n","print(mini_scan_vals)"],"metadata":{"id":"2detnktO7B60","executionInfo":{"status":"ok","timestamp":1741583401764,"user_tz":-480,"elapsed":5,"user":{"displayName":"Group D SC201 OCT24","userId":"05617796782478803590"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"e41d6a14-781f-44bd-a5e7-2f39d3f34335"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<torch.utils.data.dataloader.DataLoader object at 0x7df8b21aca10>\n","<torch.utils.data.dataloader.DataLoader object at 0x7df8bd0674d0>\n"]}]},{"cell_type":"code","source":["x, y = next(iter(mini_scan_trains))\n","print(x.shape, y.shape)"],"metadata":{"id":"ET56vqDg7FEl","executionInfo":{"status":"ok","timestamp":1741583425886,"user_tz":-480,"elapsed":24120,"user":{"displayName":"Group D SC201 OCT24","userId":"05617796782478803590"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"aa41fbd2-dd2a-444c-c52b-fc2bd12865ac"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([8, 50, 1, 224, 224]) torch.Size([8])\n"]}]},{"cell_type":"code","source":["y"],"metadata":{"id":"G_WN-NK27I69","executionInfo":{"status":"ok","timestamp":1741583425892,"user_tz":-480,"elapsed":7,"user":{"displayName":"Group D SC201 OCT24","userId":"05617796782478803590"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"d37d746d-4e37-4a80-fdb1-a8e26c44f3a5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([1, 0, 0, 1, 1, 0, 0, 1])"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torchvision.models as models\n","\n","class CNN_LSTM(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        # Feature extractor: ResNet-18 (Modified for grayscale input)\n","        self.resnet = models.resnet18(pretrained=True)\n","        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)  # 1-channel input\n","        num_flatten = self.resnet.fc.in_features\n","        self.resnet.fc = nn.Linear(num_flatten, 64)  # Feature output: 64D\n","\n","        # LSTM for temporal modeling\n","        self.lstm = nn.LSTM(input_size=64, hidden_size=256, batch_first=True)\n","\n","        # Final classification layer\n","        self.fc = nn.Linear(256, 2)  # 2 classes\n","\n","    def forward(self, x):\n","        \"\"\"\n","        x: (Batch, Slices, Channels, Height, Width) -> (N, F, C, H, W)\n","        \"\"\"\n","        N, F, C, H, W = x.shape\n","\n","        features = []  # Store ResNet output for each frame\n","\n","        for i in range(F):\n","            frame = x[:, i, :, :, :]  # (N, C, H, W) for frame i\n","            out = self.resnet(frame)  # (N, 64)\n","            features.append(out.unsqueeze(1))  # (N, 1, 64)\n","\n","        # Concatenate all frame features → (N, F, 64)\n","        out = torch.cat(features, dim=1)\n","\n","        # Pass through LSTM\n","        output, (h_n, c_n) = self.lstm(out)\n","\n","        # Use the last LSTM output (N, 256)\n","        out = output[:, -1, :]\n","\n","        # Final classification layer\n","        out = self.fc(out)  # (N, 2)\n","\n","        return out\n"],"metadata":{"id":"WHlZg5Ik7Pw9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = CNN_LSTM()"],"metadata":{"id":"LuGp7wRq7Ycs","executionInfo":{"status":"ok","timestamp":1741583426313,"user_tz":-480,"elapsed":408,"user":{"displayName":"Group D SC201 OCT24","userId":"05617796782478803590"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"7cfc7c58-e014-43ac-d259-eed8a450195f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","100%|██████████| 44.7M/44.7M [00:00<00:00, 226MB/s]\n"]}]},{"cell_type":"code","source":["model = model.to(device)"],"metadata":{"id":"3Ei1I24j7ZKF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch.optim as optim\n","loss_function = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=1e-4)\n","NUM_EPOCHS = 10\n","PRINT_EVERY = 10"],"metadata":{"id":"LSFQOle97gJt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def val(mini_scan_vals, model, device):\n","  model.eval()\n","  with torch.no_grad():\n","    total = 0\n","    for x, y in mini_scan_vals:\n","      x = x.to(device)\n","      y = y.to(device)\n","      scores = model(x)\n","      predictions = scores.argmax(axis=1)\n","      acc = predictions.eq(y).sum().item()\n","      total += acc\n","    val_acc = total / NUM_SCAN_VAL\n","    print('Val Acc:', val_acc)\n","    return val_acc"],"metadata":{"id":"hAJesGSm7jBE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["TRAIN_LOSS = []\n","VAL_ACC_LIST = []\n","BATCHES = []\n","\n","def train(mini_scan_trains, model, loss_function, optimizer, device, mini_vals):\n","  global TRAIN_LOSS, VAL_ACC_LIST, BATCHES\n","  batch_count = 0\n","  for epoch in range(NUM_EPOCHS):\n","    for count, (x, y) in enumerate(mini_scan_trains):\n","      model.train()\n","      x = x.to(device)\n","      y = y.to(device)\n","      scores = model(x)\n","      loss = loss_function(scores, y)\n","      TRAIN_LOSS.append(loss.item())\n","      BATCHES.append(batch_count)\n","      if count % PRINT_EVERY == 0:\n","        print('Training loss:', loss.item(), end = ' / ')\n","        val_acc = val(mini_scan_vals, model, device)\n","        VAL_ACC_LIST.append(val_acc)\n","      batch_count += 1\n","      optimizer.zero_grad()\n","      loss.backward()\n","      optimizer.step()"],"metadata":{"id":"-xrIYKh97jxd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train(mini_scan_trains, model, loss_function, optimizer, device, mini_scan_vals)"],"metadata":{"id":"EsRT-R8u7oSt","executionInfo":{"status":"ok","timestamp":1741604058672,"user_tz":-480,"elapsed":20632082,"user":{"displayName":"Group D SC201 OCT24","userId":"05617796782478803590"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"fc832a05-d5db-416d-b62d-1dbb30954f15"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training loss: 0.6910021305084229 / Val Acc: 0.51\n","Training loss: 0.7572619318962097 / Val Acc: 0.58\n","Training loss: 0.6188276410102844 / Val Acc: 0.565\n","Training loss: 0.6811545491218567 / Val Acc: 0.62\n","Training loss: 0.5175712704658508 / Val Acc: 0.67\n","Training loss: 0.552336573600769 / Val Acc: 0.71\n","Training loss: 0.36247357726097107 / Val Acc: 0.695\n","Training loss: 0.528309166431427 / Val Acc: 0.635\n","Training loss: 0.7364122867584229 / Val Acc: 0.675\n","Training loss: 0.8677643537521362 / Val Acc: 0.7\n","Training loss: 0.48966965079307556 / Val Acc: 0.65\n","Training loss: 0.5960067510604858 / Val Acc: 0.665\n","Training loss: 0.6547960042953491 / Val Acc: 0.695\n","Training loss: 0.5239405035972595 / Val Acc: 0.67\n","Training loss: 0.3068825304508209 / Val Acc: 0.665\n","Training loss: 0.7026999592781067 / Val Acc: 0.695\n","Training loss: 0.5422991514205933 / Val Acc: 0.63\n","Training loss: 0.6478659510612488 / Val Acc: 0.615\n","Training loss: 0.4264119267463684 / Val Acc: 0.67\n","Training loss: 0.49599581956863403 / Val Acc: 0.715\n","Training loss: 1.0328996181488037 / Val Acc: 0.685\n","Training loss: 0.6471712589263916 / Val Acc: 0.65\n","Training loss: 0.41822153329849243 / Val Acc: 0.68\n","Training loss: 0.42463991045951843 / Val Acc: 0.635\n","Training loss: 0.43611839413642883 / Val Acc: 0.67\n","Training loss: 0.5660440921783447 / Val Acc: 0.595\n","Training loss: 0.877461314201355 / Val Acc: 0.665\n","Training loss: 0.660830020904541 / Val Acc: 0.705\n","Training loss: 0.44880181550979614 / Val Acc: 0.72\n","Training loss: 0.4377119839191437 / Val Acc: 0.715\n","Training loss: 0.34812039136886597 / Val Acc: 0.72\n","Training loss: 0.7579107284545898 / Val Acc: 0.695\n","Training loss: 0.6959159970283508 / Val Acc: 0.68\n","Training loss: 0.25613653659820557 / Val Acc: 0.705\n","Training loss: 0.6268977522850037 / Val Acc: 0.665\n","Training loss: 0.3516494035720825 / Val Acc: 0.645\n","Training loss: 0.7831399440765381 / Val Acc: 0.705\n","Training loss: 0.5984330773353577 / Val Acc: 0.7\n","Training loss: 0.8069930076599121 / Val Acc: 0.7\n","Training loss: 0.7910308241844177 / Val Acc: 0.665\n","Training loss: 0.7822915315628052 / Val Acc: 0.685\n","Training loss: 0.32931768894195557 / Val Acc: 0.695\n","Training loss: 0.6498251557350159 / Val Acc: 0.735\n","Training loss: 0.677301287651062 / Val Acc: 0.735\n","Training loss: 0.5371628999710083 / Val Acc: 0.665\n","Training loss: 0.32165801525115967 / Val Acc: 0.73\n","Training loss: 0.19880254566669464 / Val Acc: 0.69\n","Training loss: 0.2190156877040863 / Val Acc: 0.685\n","Training loss: 0.2432764768600464 / Val Acc: 0.765\n","Training loss: 0.2511926293373108 / Val Acc: 0.72\n","Training loss: 0.7502161264419556 / Val Acc: 0.73\n","Training loss: 0.6288447380065918 / Val Acc: 0.64\n","Training loss: 0.7767349481582642 / Val Acc: 0.71\n","Training loss: 0.3132583498954773 / Val Acc: 0.735\n","Training loss: 0.23513193428516388 / Val Acc: 0.735\n","Training loss: 0.406472772359848 / Val Acc: 0.69\n","Training loss: 0.33285456895828247 / Val Acc: 0.74\n","Training loss: 0.7011381983757019 / Val Acc: 0.665\n","Training loss: 0.5106678009033203 / Val Acc: 0.68\n","Training loss: 0.6420246958732605 / Val Acc: 0.64\n","Training loss: 0.7420764565467834 / Val Acc: 0.695\n","Training loss: 0.47305765748023987 / Val Acc: 0.67\n","Training loss: 0.36062002182006836 / Val Acc: 0.655\n","Training loss: 0.44003498554229736 / Val Acc: 0.66\n","Training loss: 0.3256935477256775 / Val Acc: 0.695\n","Training loss: 0.4760681688785553 / Val Acc: 0.68\n","Training loss: 0.654186487197876 / Val Acc: 0.68\n","Training loss: 0.4937814772129059 / Val Acc: 0.725\n","Training loss: 0.9992718696594238 / Val Acc: 0.705\n","Training loss: 0.419836163520813 / Val Acc: 0.655\n","Training loss: 0.4245833158493042 / Val Acc: 0.665\n","Training loss: 0.7428668141365051 / Val Acc: 0.67\n","Training loss: 0.46040332317352295 / Val Acc: 0.715\n","Training loss: 0.4613843262195587 / Val Acc: 0.73\n","Training loss: 0.879400908946991 / Val Acc: 0.7\n","Training loss: 0.3981073796749115 / Val Acc: 0.71\n","Training loss: 0.4508357644081116 / Val Acc: 0.68\n","Training loss: 0.3871415853500366 / Val Acc: 0.675\n","Training loss: 0.7523377537727356 / Val Acc: 0.685\n","Training loss: 0.5284721851348877 / Val Acc: 0.73\n","Training loss: 0.1547074317932129 / Val Acc: 0.745\n","Training loss: 0.42867428064346313 / Val Acc: 0.73\n","Training loss: 0.372726172208786 / Val Acc: 0.705\n","Training loss: 0.4439096450805664 / Val Acc: 0.715\n","Training loss: 0.2368939369916916 / Val Acc: 0.745\n","Training loss: 0.2387852519750595 / Val Acc: 0.74\n","Training loss: 0.640752911567688 / Val Acc: 0.725\n","Training loss: 0.7236602306365967 / Val Acc: 0.775\n","Training loss: 0.5352371335029602 / Val Acc: 0.725\n","Training loss: 0.6700600385665894 / Val Acc: 0.705\n","Training loss: 0.5804800987243652 / Val Acc: 0.695\n","Training loss: 0.46059948205947876 / Val Acc: 0.705\n","Training loss: 0.1788465976715088 / Val Acc: 0.7\n","Training loss: 0.1728818118572235 / Val Acc: 0.7\n","Training loss: 0.13529489934444427 / Val Acc: 0.71\n","Training loss: 0.34232452511787415 / Val Acc: 0.705\n","Training loss: 0.22153739631175995 / Val Acc: 0.705\n","Training loss: 0.6648456454277039 / Val Acc: 0.645\n","Training loss: 0.27853167057037354 / Val Acc: 0.74\n","Training loss: 0.15528282523155212 / Val Acc: 0.7\n"]}]}]}