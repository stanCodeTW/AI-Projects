{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install gensim\n","import os\n","os._exit(0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0NRYlVjXm8Rs","outputId":"ff8493e8-35f3-41e4-e15b-880955765f94"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n","Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n","Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount(\"/content/drive\", force_remount=True)"],"metadata":{"id":"DRCkskUANSci","executionInfo":{"status":"ok","timestamp":1743094960093,"user_tz":-480,"elapsed":4416,"user":{"displayName":"Group-b","userId":"14709304408690978899"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"e1c8210c-33f5-4bdb-8b6a-3622ae3a4a27"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["pip install -U langchain-community neo4j flair langchain_openai PyMuPDF pdfplumber pytesseract"],"metadata":{"id":"KtJuZfHxKszT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import gc\n","from bs4 import BeautifulSoup\n","import re\n","from langchain.document_loaders import TextLoader\n","from langchain.schema import Document\n","import numpy as np\n","import logging\n","from typing import List\n","import time\n","import flair\n","from flair.data import Sentence\n","from flair.models import SequenceTagger\n","from neo4j import GraphDatabase\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from concurrent.futures import ProcessPoolExecutor\n","import fitz\n","import pytesseract\n","import pdfplumber\n","from langchain_openai import ChatOpenAI\n","from langchain.prompts import PromptTemplate\n","from langchain.chains import LLMChain"],"metadata":{"id":"49VvfSKrSgdM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!apt-get install tesseract-ocr"],"metadata":{"id":"hD1kHL_zKnP9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Setup flair model for entity extraction\n","tagger = SequenceTagger.load(\"flair/ner-english-ontonotes-large\")"],"metadata":{"id":"LylKuuEoKoxf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Setup API key\n","API_KEY = 'YOUR_API_KEY'\n","os.environ['OPENAI_API_KEY'] = API_KEY"],"metadata":{"id":"iSFweRVvGFUN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Setup LLM for relationship\n","llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.0)"],"metadata":{"id":"5s9bp-w6GLl-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Setup neo4j graph database driver\n","URI = \"YOUR_URI\"\n","AUTH = (\"neo4j\", \"YOUR_PASSWORD\")\n","driver = GraphDatabase.driver(URI, auth=AUTH)"],"metadata":{"id":"5S3Ovc05-1RR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# For metadata and relation consistency\n","nasdaq_100_mapping = {\n","    \"AAPL\": \"Apple Inc.\",\n","    \"MSFT\": \"Microsoft Corporation\",\n","    \"AMZN\": \"Amazon.com, Inc.\",\n","    \"GOOG\": \"Alphabet Inc. (Class C)\",\n","    \"GOOGL\": \"Alphabet Inc. (Class A)\",\n","    \"META\": \"Meta Platforms, Inc.\",\n","    \"NVDA\": \"NVIDIA Corporation\",\n","    \"TSLA\": \"Tesla, Inc.\",\n","    \"PEP\": \"PepsiCo, Inc.\",\n","    \"COST\": \"Costco Wholesale Corporation\",\n","    \"ADBE\": \"Adobe Inc.\",\n","    \"CMCSA\": \"Comcast Corporation\",\n","    \"CSCO\": \"Cisco Systems, Inc.\",\n","    \"AVGO\": \"Broadcom Inc.\",\n","    \"INTC\": \"Intel Corporation\",\n","    \"NFLX\": \"Netflix, Inc.\",\n","    \"TXN\": \"Texas Instruments Incorporated\",\n","    \"QCOM\": \"QUALCOMM Incorporated\",\n","    \"INTU\": \"Intuit Inc.\",\n","    \"AMGN\": \"Amgen Inc.\",\n","    \"SBUX\": \"Starbucks Corporation\",\n","    \"ISRG\": \"Intuitive Surgical, Inc.\",\n","    \"MDLZ\": \"Mondelez International, Inc.\",\n","    \"BKNG\": \"Booking Holdings Inc.\",\n","    \"ADI\": \"Analog Devices, Inc.\",\n","    \"PYPL\": \"PayPal Holdings, Inc.\",\n","    \"ADP\": \"Automatic Data Processing, Inc.\",\n","    \"LRCX\": \"Lam Research Corporation\",\n","    \"GILD\": \"Gilead Sciences, Inc.\",\n","    \"FISV\": \"Fiserv, Inc.\",\n","    \"VRTX\": \"Vertex Pharmaceuticals Incorporated\",\n","    \"REGN\": \"Regeneron Pharmaceuticals, Inc.\",\n","    \"KDP\": \"Keurig Dr Pepper Inc.\",\n","    \"ASML\": \"ASML Holding N.V.\",\n","    \"ORLY\": \"O'Reilly Automotive, Inc.\",\n","    \"MNST\": \"Monster Beverage Corporation\",\n","    \"MELI\": \"MercadoLibre, Inc.\",\n","    \"CTSH\": \"Cognizant Technology Solutions Corporation\",\n","    \"SNPS\": \"Synopsys, Inc.\",\n","    \"PANW\": \"Palo Alto Networks, Inc.\",\n","    \"CDNS\": \"Cadence Design Systems, Inc.\",\n","    \"MAR\": \"Marriott International, Inc.\",\n","    \"ROST\": \"Ross Stores, Inc.\",\n","    \"IDXX\": \"IDEXX Laboratories, Inc.\",\n","    \"EXC\": \"Exelon Corporation\",\n","    \"CSX\": \"CSX Corporation\",\n","    \"MCHP\": \"Microchip Technology Incorporated\",\n","    \"AEP\": \"American Electric Power Company, Inc.\",\n","    \"XEL\": \"Xcel Energy Inc.\",\n","    \"DLTR\": \"Dollar Tree, Inc.\",\n","    \"PAYX\": \"Paychex, Inc.\",\n","    \"WBA\": \"Walgreens Boots Alliance, Inc.\",\n","    \"CTAS\": \"Cintas Corporation\",\n","    \"PCAR\": \"PACCAR Inc\",\n","    \"ODFL\": \"Old Dominion Freight Line, Inc.\",\n","    \"FAST\": \"Fastenal Company\",\n","    \"ANSS\": \"ANSYS, Inc.\",\n","    \"VRSK\": \"Verisk Analytics, Inc.\",\n","    \"BIIB\": \"Biogen Inc.\",\n","    \"CHTR\": \"Charter Communications, Inc.\",\n","    \"EA\": \"Electronic Arts Inc.\",\n","    \"SIRI\": \"Sirius XM Holdings Inc.\",\n","    \"ILMN\": \"Illumina, Inc.\",\n","    \"WBD\": \"Warner Bros. Discovery, Inc.\",\n","    \"ZM\": \"Zoom Video Communications, Inc.\",\n","    \"KHC\": \"The Kraft Heinz Company\",\n","    \"ALGN\": \"Align Technology, Inc.\",\n","    \"CEG\": \"Constellation Energy Corporation\",\n","    \"DDOG\": \"Datadog, Inc.\",\n","    \"DOCU\": \"DocuSign, Inc.\",\n","    \"EBAY\": \"eBay Inc.\",\n","    \"ENPH\": \"Enphase Energy, Inc.\",\n","    \"FANG\": \"Diamondback Energy, Inc.\",\n","    \"FTNT\": \"Fortinet, Inc.\",\n","    \"GEHC\": \"GE HealthCare Technologies Inc.\",\n","    \"GFS\": \"GLOBALFOUNDRIES Inc.\",\n","    \"HON\": \"Honeywell International Inc.\",\n","    \"KLAC\": \"KLA Corporation\",\n","    \"LULU\": \"Lululemon Athletica Inc.\",\n","    \"MSTR\": \"MicroStrategy Incorporated\",\n","    \"MU\": \"Micron Technology, Inc.\",\n","    \"NXPI\": \"NXP Semiconductors N.V.\",\n","    \"ON\": \"ON Semiconductor Corporation\",\n","    \"PANW\": \"Palo Alto Networks, Inc.\",\n","    \"PDD\": \"Pinduoduo Inc.\",\n","    \"PLTR\": \"Palantir Technologies Inc.\",\n","    \"ROK\": \"Rockwell Automation, Inc.\",\n","    \"SGEN\": \"Seagen Inc.\",\n","    \"SPLK\": \"Splunk Inc.\",\n","    \"TEAM\": \"Atlassian Corporation\",\n","    \"TTD\": \"The Trade Desk, Inc.\",\n","    \"VRSN\": \"VeriSign, Inc.\",\n","    \"WDAY\": \"Workday, Inc.\",\n","    \"ZS\": \"Zscaler, Inc.\"\n","}"],"metadata":{"id":"fQa6aEaeP-Ci"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Load documents**"],"metadata":{"id":"PWvWM76wIZAS"}},{"cell_type":"code","source":["# PDF Document Directory\n","PDF_PATH = '/content/drive/MyDrive/Colab Notebooks/transcripts/'"],"metadata":{"id":"GorD4sWJNYzG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Document:\n","  def __init__(self, page_content: str, metadata: dict = None):\n","    self.page_content = page_content\n","    self.metadata = metadata if metadata else {}\n","\n","class PDFLoader(TextLoader):\n","  def __init__(self, file_path):\n","    self.file_path = file_path\n","    self.metadata = self.extract_metadata(file_path)\n","\n","  def extract_metadata(self, file_path: str) -> dict:\n","    company_ticker = os.path.basename(os.path.dirname(file_path))\n","    return {'source': 'Transcript', 'company_ticker': company_ticker}\n","\n","  def extract_text_from_pdf(self, pdf_path: str) -> str:\n","    doc = fitz.open(pdf_path)\n","    text = \"\"\n","    for page in doc:\n","        text += page.get_text(\"text\")\n","    return text\n","\n","  def extract_text_from_image_pdf(self, pdf_path: str) -> str:\n","    text = \"\"\n","    try:\n","      with pdfplumber.open(pdf_path) as pdf:\n","        for page in pdf.pages:\n","          pix = page.to_image()\n","          img = pix.original\n","          text += pytesseract.image_to_string(img)  # OCR extraction\n","    except Exception as e:\n","      print(f\"Error extracting text from {pdf_path}: {e}\")\n","    return text\n","\n","  def preprocess(self, text: str, metadata: dict) -> List[Document]:\n","    # Returns a list of Document objects\n","    clean_text = clean_text(text)\n","    return [Document(page_content=clean_text, metadata=metadata)]\n","\n","  def clean_text(self, text: str) -> str:\n","    patterns = [\n","            (r'\\b\\d{8,}\\b', ''),\n","            (r'\\b\\d{2,4}[-/\\.\\d]*\\b', ''),\n","            (r'\\s+', ' '),\n","            (r'[^a-zA-Z0-9\\s.,!?\\'\"(){}-]', ''),\n","        ]\n","    for pattern, replacement in patterns:\n","      text = re.sub(pattern, replacement, text)\n","    return text.strip()\n"],"metadata":{"id":"KRlysBMqNq9J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load(filepath) -> List[tuple] :\n","  files = []\n","  for root, dirs, files_in_dir in os.walk(filepath):\n","    for file_name in files_in_dir:\n","      if file_name.lower().endswith('.pdf'):\n","        company_ticker = os.path.basename(root)\n","        file_path = os.path.join(root, file_name)\n","        files.append((file_path, company_ticker))\n","  return files"],"metadata":{"id":"sxRLmU8Mcu5E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_all_docs(file_path: str) -> List[Document]:\n","  pdf_loader = PDFLoader(file_path)\n","  pdf_files = load(file_path)  # This returns a list of tuples (file_path, company_ticker)\n","\n","  all_docs = []\n","  for pdf_file, company_ticker in pdf_files:\n","    print(f\"Processing {pdf_file}...\")\n","\n","    # Extract metadata for the document\n","    metadata = pdf_loader.extract_metadata(pdf_file)\n","\n","    # Try to extract text normally first (text-based PDFs)\n","    text = pdf_loader.extract_text_from_pdf(pdf_file)\n","\n","    # If no text was extracted, use OCR (image-based PDFs)\n","    if not text.strip():\n","      print(f\"Using OCR for {pdf_file}...\")\n","      text = pdf_loader.extract_text_from_image_pdf(pdf_file)\n","\n","    # Preprocess the extracted text and metadata into Document objects\n","    documents = pdf_loader.preprocess(text, metadata)\n","\n","    # Add all the processed Document objects to the all_docs list\n","    all_docs.extend(documents)\n","\n","  return all_docs"],"metadata":{"id":"NcFgFOEXNtwX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["all_docs = load_all_docs(PDF_PATH)"],"metadata":{"id":"S6eu6IqmItbw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Split doc into chunks**"],"metadata":{"id":"g8SIUrKTI2ZJ"}},{"cell_type":"code","source":["text_splitter = RecursiveCharacterTextSplitter(\n","    chunk_size=1000,\n","    chunk_overlap=200,\n","    separators=[\"\\n\\n\", \"\\n\", \" \", \",\"],\n","    add_start_index = True\n",")\n","split_documents = text_splitter.split_documents(all_docs)"],"metadata":{"id":"qHjU8AdjKsax"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Extract entities from text within chunks**"],"metadata":{"id":"ym4J5qrwI_4a"}},{"cell_type":"code","source":["# FINANCIAL_TERMS not applicable to our current usage\n","# FINANCIAL_TERMS = {\n","#     \"Revenue\": [\"Revenue\", \"Net Sales\"],\n","#     \"EBITDA\": [\"EBITDA\"],\n","#     \"FCF\": [\"Free Cash Flow\"],\n","#     \"Net Income\": [\"Net Income\", \"Net Profit\"],\n","#     \"Operating Income\": [\"Operating Income\", \"Operating Profit\"],\n","#     \"Gross Profit\": [\"Gross Profit\"],\n","#     'ACV': ['ACV']\n","#     }\n","EXECUTIVE_TITLES = [\"CEO\", \"Chairman\", \"President\", \"Director\"]\n","PRODUCT_PATTERNS = [\n","    r\"our (?:products|services) include:?\\s*(.*?)[\\.;]\",  # \"Our products include: [].\"\n","    r\"we offer\\s*(.*?)[\\.;]\",  # \"We offer [].\"\n","    r\"we provide\\s*(.*?)[\\.;]\",  # \"We provide [] services.\"\n","    ]\n","GEO_PATTERNS = {\n","    \"Headquarters\": r\"headquartered in ([A-Za-z\\s,]+)\",\n","    \"Operating Regions\": r\"operate in ([A-Za-z\\s,]+)\",\n","    \"Markets\": r\"market[s]? include[s]? ([A-Za-z\\s,]+)\",\n","}\n","EVENT_PATTERNS = {\n","    \"Mergers & Acquisitions\": r\"acquired\\s+([A-Za-z\\s]+)\\s+for\\s+\\$?[\\d,.]+\",\n","    \"Product Launches\": r\"launched\\s+([A-Za-z\\s]+)\\s+in\\s+\\d{4}\",\n","    \"Earnings Calls\": r\"our\\s+Q\\d\\s+earnings\\s+call\",\n","}\n","LEGAL_PATTERNS = {\n","    \"Legal Cases\": r\"(lawsuit|litigation|antitrust case|settlement)\"\n","}"],"metadata":{"id":"WvnpFPmxGtRG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# For financial data, not applicable to our current usage\n","UNIT_CONVERSION = {\n","    \"billion\": 1000,  # Billion → Convert to million\n","    \"million\": 1,  # Million\n","    \"thousand\": 0.001  # Thousand → Convert to million\n","}\n","def normalize_number(value, unit):\n","  \"\"\"Convert financial numbers into standardized millions.\"\"\"\n","  num = float(value.replace(\",\", \"\"))  # Remove commas\n","  multiplier = UNIT_CONVERSION.get(unit.lower(), 1)  # Default to million if missing\n","  return round(num * multiplier, 3)  # Keep 3 decimal places"],"metadata":{"id":"yjz7cEWdLyJK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def extract_entities(text):\n","\n","  sentence = Sentence(text)\n","  tagger.predict(sentence)\n","\n","  # # For financial data, not applicable to our current usage\n","  # extracted = {key: [] for key in FINANCIAL_TERMS}\n","  extracted={\n","      \"Company\": [],\n","      \"Executives\": [],\n","      \"Products\": [],\n","      \"Geo\": {\"Headquarters\": [], \"Operating Regions\": [], \"Markets\": []},\n","      \"Events\": {\"Mergers & Acquisitions\": [], \"Product Launches\": [], \"Earnings Calls\": []},\n","      \"Legal Issues\": {\"Legal Cases\": []},\n","    }\n","\n","  detected_people = []\n","  detected_title = []\n","  detected_products = []\n","  detected_geo = []\n","  detected_legal = []\n","\n","  for entity in sentence.get_spans(\"ner\"):\n","    if entity.tag == \"ORG\":  # Company names\n","      if entity.text != 'Company':\n","        extracted[\"Company\"].append(entity.text)\n","    # # For financial data, not applicable to our current usage\n","    # elif entity.tag == \"MONEY\":  # Financial numbers (Revenue, Market Cap)\n","    #   money_value = entity.text.replace(\"$\", \"\").replace(\",\", \"\").strip()\n","    #   if money_value.isnumeric():  # Ensure it's a valid number\n","    #     extracted.setdefault(\"MONEY\", []).append(money_value)\n","    elif entity.tag in [\"PRODUCT\", \"WORK_OF_ART\"]:\n","      detected_products.append(entity.text)\n","    elif entity.tag == \"GPE\":\n","      detected_geo.append(entity.text)\n","    elif entity.tag == \"PERSON\":  # Executives\n","      detected_people.append(entity.text)\n","    elif entity.tag == \"LAW\":  # Legal references\n","      detected_legal.append(entity.text)\n","\n","  # For financial data, not applicable to our current usage\n","  # for key, variations in FINANCIAL_TERMS.items():\n","  #   for term in variations:\n","  #     match = re.search(rf\"({term})[^$]*\\$\\s?([\\d,.]+)\\s*(billion|million|thousand)?\", text, re.IGNORECASE)\n","  #     if match:\n","  #       value, unit = match.group(2), match.group(3)\n","  #       extracted[key].append(str(normalize_number(value, unit)) + \" million\")\n","\n","  # Match Executive\n","  for title in EXECUTIVE_TITLES:\n","    match = re.search(rf\"([\\w\\s]+)\\s+is\\s+(?:the\\s+)?{title}\\s+of\\s+([\\w\\s]+)|{title}[:\\-]?\\s*([\\w\\s]+)\\s*\\(?([\\w\\s]*)\\)?\", text, re.IGNORECASE)\n","    if match:\n","      detected_title.append((match.group(1), match.group(2)))\n","\n","  # Extract executive and title\n","  for title, name in detected_title:\n","    # Ensure the detected name is in NER-detected people list\n","    if name in detected_people or len(detected_people) == 1:\n","      extracted[\"Executives\"].append(f\"{title}: {name}\")\n","    elif len(detected_people) > 1:\n","      extracted[\"Executives\"].append(f\"{title}: {detected_people.pop(0)}\")\n","\n","  # Extract Geo\n","  for key, pattern in GEO_PATTERNS.items():\n","    match = re.search(pattern, text, re.IGNORECASE)\n","    if match:\n","      extracted[\"Geo\"][key].append(match.group(1).strip())\n","\n","  # Extract Events\n","  for key, pattern in EVENT_PATTERNS.items():\n","    match = re.search(pattern, text, re.IGNORECASE)\n","    if match:\n","      extracted[\"Events\"][key].append(match.group(1).strip() if key != \"Earnings Calls\" else \"Detected\")\n","\n","  # Extract Legal Issues\n","  for key, pattern in LEGAL_PATTERNS.items():\n","    if re.search(pattern, text, re.IGNORECASE):\n","      extracted[\"Legal Issues\"][key].append(\"Detected\")\n","\n","  # Extract Products\n","  for pattern in PRODUCT_PATTERNS:\n","    match = re.search(pattern, text, re.IGNORECASE)\n","    if match:\n","      extracted[\"Products\"].extend([item.strip() for item in match.group(1).split(\",\")])\n","\n","  extracted[\"Geo\"][\"Headquarters\"].extend(detected_geo)\n","  extracted[\"Legal Issues\"][\"Legal Cases\"].extend(detected_legal)\n","\n","  return extracted"],"metadata":{"id":"SFnETsmlDCNg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Use LLM to generate triplet**"],"metadata":{"id":"PSJRTjJ4JibW"}},{"cell_type":"code","source":["PROMPT_TEMPLATE = \"\"\"\n","Generate the necessary data for a triplet that can be directly stored in Neo4j, only from the following extracted entities from a 10-K filing:\n","\n","Company: {Company}\n","Executives: {Executives}\n","Products: {Products}\n","Geo (Headquarters, Operating Regions, Markets): {Geo}\n","Events (Mergers & Acquisitions, Product Launches, Earnings Calls): {Events}\n","Legal Issues (Legal Cases, Regulatory Compliance): {Legal_Issues}\n","\n","The output of the triplet should be formatted as follows, which could be directly used to store in Neo4j:\n","\n","(head_entity, relationship, object_entity, entity_category)\n","\n","For example: (\"Microsoft\", \"IS_THE_CEO_OF\", \"Satya Nadella\", \"Executives\")\n","\n","However, if no relationship can be determined from a category, return exactly \"No clear relationship\" within the categoty of the output.\n","\"\"\""],"metadata":{"id":"lLSmBhYhL9Vb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Adjust based on actual situation\n","EXCLUDED_ENTITIES = {\"USGovernmentAgenciesDebtSecuritiesMember\", \"UNITED STATES SECURITIES\",'the Securities and Exchange Commission', 'Investor Relations', 'non-GAAP', 'GAAP', 'ACV'}\n","EXCLUDED_OBJECTS = {\"Executives\", \"Products\", \"Geo\", \"Events\", \"Legal Issues\", \"Unknown\", \"Markets\", 'Legal Cases', 'Headquarters', 'Operating Regions', 'Product Launches', 'Earnings Calls', 'Regulatory Compliance', 'Companys', 'The Company', 'Mergers & Acquisitions'}"],"metadata":{"id":"_kqkdTGgRn5w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_triplet(chunk):\n","  entities = extract_entities(chunk.page_content)\n","  metadata = chunk.metadata\n","\n","  # Fill prompt with data, financial data not applicable to our current usage\n","  filled_prompt = PROMPT_TEMPLATE.format(\n","      # Revenue=\", \".join(entities.get('Revenue', [])),\n","      # EBITDA=\", \".join(entities.get('EBITDA', [])),\n","      # FCF=\", \".join(entities.get('FCF', [])),\n","      # Net_Income=\", \".join(entities.get('Net Income', [])),\n","      # Operating_Income=\", \".join(entities.get('Operating Income', [])),\n","      # Gross_Profit=\", \".join(entities.get('Gross Profit', [])),\n","      # ACV=\", \".join(entities.get('ACV', [])),\n","      Company=\", \".join(entities.get('Company', [])),\n","      Executives=\", \".join(entities.get('Executives', [])),\n","      Products=\", \".join(entities.get('Products', [])),\n","      Geo=\", \".join([f\"{key}: {', '.join(val)}\" for key, val in entities.get('Geo', {}).items()]),\n","      Events=\", \".join([f\"{key}: {', '.join(val)}\" for key, val in entities.get('Events', {}).items()]),\n","      Legal_Issues=\", \".join([f\"{key}: {', '.join(val)}\" for key, val in entities.get('Legal Issues', {}).items()])\n","    )\n","\n","  # triplet generated from LLM\n","  response = llm(filled_prompt)\n","  triplet_strings = response.content.strip().split('\\n')\n","  triplets = []\n","\n","  # Add metadata\n","  formatted_metadata = \", \".join([f\"{key}: {val}\" for key, val in metadata.items()])\n","\n","  for triplet in triplet_strings:\n","    triplet = triplet.strip(\"[]\").replace('\"', '').strip()\n","    triplet_parts = triplet.split(\", \")\n","\n","    if len(triplet_parts) != 4 or \"NO_CLEAR_RELATIONSHIP\" in triplet or 'HAS_NO_INFORMATION' in triplet or 'None' in triplet:\n","      continue\n","\n","    # Adjust accordingly to prompt\n","    head_entity, relationship, object_entity, entity_category = triplet_parts\n","\n","    if head_entity in EXCLUDED_ENTITIES or object_entity in EXCLUDED_ENTITIES:\n","      continue\n","    if object_entity == \"No clear relationship\" or object_entity == \"\":\n","      continue\n","    if object_entity in EXCLUDED_OBJECTS:\n","      continue\n","    if head_entity in ['Company', 'Registrant', 'CommonStockMember']:\n","      ticker = metadata.get('company_ticker', head_entity)\n","      head_entity = nasdaq_100_mapping.get(ticker, ticker)\n","\n","    triplets.append((head_entity, relationship, object_entity, entity_category, formatted_metadata))\n","\n","  return triplets"],"metadata":{"id":"C3H9KuPLTrJk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Store triplet in neo4j database**"],"metadata":{"id":"I04AohwQKWUT"}},{"cell_type":"code","source":["def store_triplet_in_neo4j(triplet):\n","  graph = driver.session()\n","  try:\n","    metadata_str = triplet[4]\n","    metadata_dict = dict(item.split(\": \", 1) for item in metadata_str.split(\", \") if \": \" in item)\n","\n","    head_entity = triplet[0].strip(\"()\")\n","    relationship = triplet[1].strip('\"')\n","    object_entity = triplet[2].strip('\"')\n","    entity_category = triplet[3].strip('\"')\n","\n","    if relationship.lower() == \"no clear relationship\" or relationship.lower() == \"HAS_NO_INFORMATION\":\n","      return\n","\n","    if any(term in head_entity for term in [\"Company\", \"Registrant\", \"CommonStockMember\"]):\n","      ticker = metadata_dict.get('company_ticker', head_entity)\n","      head_entity = nasdaq_100_mapping.get(ticker, ticker)\n","    if any(term in object_entity for term in [\"Company\", \"Registrant\", \"CommonStockMember\"]):\n","      ticker = metadata_dict.get('company_ticker', object_entity)\n","      object_entity = nasdaq_100_mapping.get(ticker, ticker)\n","    if head_entity in EXCLUDED_ENTITIES or object_entity in EXCLUDED_ENTITIES:\n","      return\n","\n","    with driver.session() as session:\n","      cypher_query = \"\"\"\n","      MERGE (head:Entity {name: $head_entity})\n","      SET head.category = $category\n","      \"\"\" + \"\".join([f\" SET head.{key} = '{value}'\" for key, value in metadata_dict.items()]) + \"\"\"\n","\n","      MERGE (object:Entity {name: $object_entity})\n","      SET object.category = $category\n","\n","      MERGE (head)-[r:\"\"\" + relationship + \"\"\"]->(object)\n","      \"\"\"\n","\n","      session.run(\n","          cypher_query,\n","          head_entity=head_entity,\n","          object_entity=object_entity,\n","          category=entity_category\n","          )\n","\n","      print(f\"Stored triplet: ({head_entity}) -[{relationship}]-> ({object_entity}) with metadata {metadata_dict}\")\n","\n","  except Exception as e:\n","    print(f\"Error while storing triplet: {triplet}. Error: {e}\")"],"metadata":{"id":"XrLpnKr2db2_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for doc in split_documents:\n","  triplets = generate_triplet(doc)\n","  for triplet in triplets:\n","    store_triplet_in_neo4j(triplet)"],"metadata":{"id":"H6oDLEyZJPpu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"f3vNJyPOlLTl"},"execution_count":null,"outputs":[]}]}