{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount(\"/content/drive\", force_remount=True)"],"metadata":{"id":"DRCkskUANSci","executionInfo":{"status":"ok","timestamp":1743167105016,"user_tz":-480,"elapsed":19290,"user":{"displayName":"Group-b","userId":"14709304408690978899"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"85533e7e-a5e7-4a99-c9b5-3156207942d9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["pip install -U langchain-community langchain_openai tiktoken chromadb"],"metadata":{"id":"wKAbDg5Joy1T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import gc\n","from bs4 import BeautifulSoup\n","import re\n","from langchain.document_loaders import TextLoader\n","from langchain.schema import Document\n","import numpy as np\n","from concurrent.futures import ProcessPoolExecutor\n","import logging\n","from typing import List\n","import time\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","import concurrent.futures\n","from langchain.embeddings.openai import OpenAIEmbeddings\n","from langchain.vectorstores import Chroma\n","import openai"],"metadata":{"id":"49VvfSKrSgdM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Docuemnt directory to load and do text embedding\n","FILEPATH = '/content/drive/MyDrive/Colab Notebooks/financial_reports/'"],"metadata":{"id":"GorD4sWJNYzG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Document:\n","  def __init__(self, page_content: str, metadata: dict = None):\n","    self.page_content = page_content\n","    self.metadata = metadata if metadata else {}\n","\n","class HTMLTextLoader(TextLoader):\n","  def __init__(self, file_path):\n","    self.file_path = file_path\n","    self.metadata = self.extract_metadata(file_path)\n","\n","  def extract_metadata(self, file_path):\n","    # Adjust based on actual filing structure within the directory\n","    company_ticker = os.path.basename(os.path.dirname(os.path.dirname(self.file_path)))\n","    year = os.path.basename(os.path.dirname(self.file_path)).split('-')[1]\n","    return {'file_path': file_path, 'company_ticker': company_ticker, 'year': year}\n","\n","  def preprocess(self, content) -> List[Document]:\n","    try:\n","      soup = BeautifulSoup(content, 'html.parser')\n","      body = soup.find('body')\n","\n","    except Exception as e:\n","      logging.error(f\"Error parsing HTML for {self.file_path}: {e}\")\n","      return []\n","\n","    if body is None:\n","      logging.warning(f\"No <body> tag found in {self.file_path}\")\n","      return []\n","\n","    # Removes scripts and styles\n","    for script_or_style in body(['script', 'style']):\n","      script_or_style.decompose()\n","\n","    clean_text = body.get_text(separator=' ', strip=True)\n","    clean_text = self.clean_text(clean_text)\n","\n","    return [Document(page_content=clean_text, metadata=self.metadata)]\n","\n","  def clean_text(self, text: str) -> str:\n","    # Adjust based on actual situation\n","    patterns = [\n","        (r'(us-gaap|xbrli|srt|P\\d{1,2}Y)', ''),\n","         (r'\\b\\d{8,}\\b', ''),\n","          (r'\\b\\d{2,4}[-/\\.\\d]*\\b', ''),\n","           (r'\\s+', ' '),\n","            (r'[^a-zA-Z0-9\\s.,!?\\'\"(){}-]', '')\n","            ]\n","    for pattern, replacement in patterns:\n","      text = re.sub(pattern, replacement, text)\n","\n","    return text.strip()\n"],"metadata":{"id":"KRlysBMqNq9J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load(filepath) -> List[tuple]:\n","  files = []\n","  # Adjust based on actual filing structure within the directory\n","  for root, dirs, files_in_dir in os.walk(filepath):\n","    for file_name in files_in_dir:\n","      # Adjust based on actual naming rules\n","      if file_name == 'full-submission.txt':\n","        company_ticker = os.path.basename(os.path.dirname(root))\n","        year = os.path.basename(root).split('-')[1]\n","        file_path = os.path.join(root, file_name)\n","        files.append((file_path, company_ticker, year))\n","  return files"],"metadata":{"id":"sxRLmU8Mcu5E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def process_file(file_info):\n","    file_path, company_ticker, year = file_info\n","    loader = HTMLTextLoader(file_path)\n","\n","    with open(file_path, 'r', encoding='utf-8') as f:\n","      content = f.read()\n","\n","    return loader.preprocess(content)\n","\n","def load_all_docs(file_path) -> List[Document]:\n","  all_files = load(file_path)\n","  all_docs = []\n","\n","  with ProcessPoolExecutor() as executor:\n","    all_docs = list(executor.map(process_file, all_files))\n","  return [doc for sublist in all_docs for doc in sublist]"],"metadata":{"id":"NcFgFOEXNtwX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["all_docs = load_all_docs(FILEPATH)"],"metadata":{"id":"aK3chUeS4Ov7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text_splitter = RecursiveCharacterTextSplitter(\n","    chunk_size=1000,\n","    chunk_overlap=200,\n","    separators=[\"\\n\\n\", \"\\n\", \" \", \",\"],\n","    add_start_index = True\n",")\n","\n","# Split documents into chunks\n","split_documents = text_splitter.split_documents(all_docs)"],"metadata":{"id":"vUMT8B8NpXli"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Setup API key for text embedding\n","API_KEY = 'YOUR_API_KEY'\n","os.environ['OPENAI_API_KEY'] = API_KEY"],"metadata":{"id":"FlQR6sxRlN-k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Setup Chroma database path and text embedding model\n","CHROMA_PATH = '/content/drive/FinScope3D/Unstructured_Data/chroma_db'\n","embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")"],"metadata":{"id":"TmLy6Hqf4m92"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def batch_documents(documents, batch_size):\n","  return [documents[i:i + batch_size] for i in range(0, len(documents), batch_size)]"],"metadata":{"id":"IzyNRS2VP76X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_size = 10000\n","batched_documents = batch_documents(split_documents, batch_size)"],"metadata":{"id":"l52cHb9lSWsR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def process_and_persist_batch(batch, embeddings, persist_directory, batch_idx):\n","  text = [doc.page_content for doc in batch]\n","  embedding = embeddings.embed_documents(text)\n","  db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embeddings)\n","\n","  db.add_documents(batch, embeddings=embedding)\n","  db.persist()\n","  print(f\"Batch {batch_idx + 1} processed and persisted.\")"],"metadata":{"id":"nQJvgbaN64aj"},"execution_count":null,"outputs":[]}]}